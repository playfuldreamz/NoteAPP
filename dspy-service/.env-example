# DSPy Service Environment Variables
# Copy this file to .env and update with your values

# LLM Provider Configuration
DSPY_LLM_PROVIDER=google  # Options: openai, google, anthropic, ollama, lmstudio
DSPY_LLM_MODEL=gemini-pro  # Example: gpt-4o-mini, gemini-pro, claude-3-opus

# API Keys based on provider
# OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_google_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# For Local LLMs (Ollama/LMStudio)
# LOCAL_LLM_BASE_URL=http://localhost:11434/v1

# Node.js Backend URL (for tools to call back to)
NODE_BACKEND_URL=http://localhost:5000

# Server Configuration
HOST=localhost
PORT=5001

# Logging
LOG_LEVEL=info
